{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn . metrics import confusion_matrix \n",
    "import random\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "\n",
    "CLASS=15 #col no of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(filename): \n",
    "    #trainSet = []\n",
    "    #testSet = []\n",
    "    dataset=pd.read_csv(filename)\n",
    "    dataset.drop(columns=\"pred\")\n",
    "    #dataset.drop(columns=\"class\")\n",
    "    dataset.drop(columns=\"spectrometric_redshift\")\n",
    "    \n",
    "    d=cross_validation_split(dataset,2)\n",
    "    test,train=d[0],d[1]\n",
    "    \n",
    "    return test,train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset , n_folds): \n",
    "    dataset_split = list()\n",
    "    dataset_copy = dataset.values.tolist()\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds): \n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(len(dataset_copy)) \n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold) \n",
    "\n",
    "    print(len(dataset_split))\n",
    "    #print(dataset_split)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eud(i1, i2):\n",
    "    dist=0\n",
    "    length=len(i1)\n",
    "    for x in range(2,length):\n",
    "        if(x!=CLASS):\n",
    "            dist +=pow((i1[x]-i2[x]), 2)\n",
    "    return math.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(distances):\n",
    "    one=0\n",
    "    zero=0\n",
    "    for d in distances:\n",
    "        if(d[0][CLASS]==1):\n",
    "            one+=1\n",
    "        else:\n",
    "            zero+=1\n",
    "    cl=1 if one > zero else 0\n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(test,train,k):\n",
    "    pred=[]\n",
    "    trueValue=[]\n",
    "    for t1 in test:\n",
    "        dist=[]\n",
    "        for t2 in train:\n",
    "            dist.append((t2,eud(t1,t2)))\n",
    "        dist.sort(key=lambda x:x[1],reverse=True)\n",
    "        dist=dist[0:k]\n",
    "        \n",
    "        c=get_class(dist)\n",
    "        pred.append(c)\n",
    "        trueValue.append(int(t1[CLASS]))\n",
    "    return(trueValue,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPR(cm):\n",
    "    res=cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "    return res\n",
    "\n",
    "def TNR(cm):\n",
    "    res=cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "    return res\n",
    "\n",
    "def Precision(cm):\n",
    "    pass\n",
    "\n",
    "def metrics(cm):\n",
    "    TP=cm[0][0]\n",
    "    TN=cm[1][1]\n",
    "    FP=cm[0][1]\n",
    "    FN=cm[1][0]\n",
    "\n",
    "    TPR = TP/(TP+FN)\n",
    "    print( 'Sensitivity: {}\\n'.format(TPR))\n",
    "    TNR = TN/(TN+FP)\n",
    "    print( 'Specificity: {}\\n'.format(TNR)) \n",
    "    Precision = TP/(TP+FP)\n",
    "    print ( 'Precision: {}\\n'. format ( Precision ) )\n",
    "    Recall = TP/(TP+FN)\n",
    "    print ( 'Recall: {}\\n' . format ( Recall ) ) \n",
    "    Acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    print ( 'Accuracy: {}\\n' . format (Acc) )\n",
    "    Fscore = 2*(Precision*Recall)/(Precision+Recall) \n",
    "    print ( 'FScore: {}\\n'.format(Fscore ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "\n",
      "ConfusionMatrix\n",
      "\n",
      "   0  29\n",
      "   0 295\n"
     ]
    }
   ],
   "source": [
    "test,train=loaddata('dataset/catalog1/cat1.csv')\n",
    "t,p=nb(test,train,7)\n",
    "\n",
    "cm = confusion_matrix(t,p)\n",
    "\n",
    "print ( '\\n\\nConfusionMatrix\\n' ) \n",
    "print('\\n'.join([''.join(['{:4}'.format(item) for item in row]) for row in cm])) #confusionmatrix = np.matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: nan\n",
      "\n",
      "Specificity: 0.9104938271604939\n",
      "\n",
      "Precision: 0.0\n",
      "\n",
      "Recall: nan\n",
      "\n",
      "Accuracy: 0.9104938271604939\n",
      "\n",
      "FScore: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "metrics(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
